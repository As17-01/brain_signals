{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 200\n",
    "\n",
    "ID = [\"ID\"]\n",
    "IDS = [\"SubjectID\", \"VideoID\"]\n",
    "TARGET = [\"predefinedlabel\"]\n",
    "\n",
    "FEATURES = [\n",
    "    \"Raw\",\n",
    "    \"Delta\",\n",
    "    \"Theta\",\n",
    "    \"Alpha1\",\n",
    "    \"Alpha2\",\n",
    "    \"Beta1\",\n",
    "    \"Beta2\",\n",
    "    \"Gamma1\",\n",
    "    \"Gamma2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Raw</th>\n",
       "      <th>Delta</th>\n",
       "      <th>Theta</th>\n",
       "      <th>Alpha1</th>\n",
       "      <th>Alpha2</th>\n",
       "      <th>Beta1</th>\n",
       "      <th>Beta2</th>\n",
       "      <th>Gamma1</th>\n",
       "      <th>Gamma2</th>\n",
       "      <th>predefinedlabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>301963.0</td>\n",
       "      <td>90612.0</td>\n",
       "      <td>33735.0</td>\n",
       "      <td>23991.0</td>\n",
       "      <td>27946.0</td>\n",
       "      <td>45097.0</td>\n",
       "      <td>33228.0</td>\n",
       "      <td>8293.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>73787.0</td>\n",
       "      <td>28083.0</td>\n",
       "      <td>1439.0</td>\n",
       "      <td>2240.0</td>\n",
       "      <td>2746.0</td>\n",
       "      <td>3687.0</td>\n",
       "      <td>5293.0</td>\n",
       "      <td>2740.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>758353.0</td>\n",
       "      <td>383745.0</td>\n",
       "      <td>201999.0</td>\n",
       "      <td>62107.0</td>\n",
       "      <td>36293.0</td>\n",
       "      <td>130536.0</td>\n",
       "      <td>57243.0</td>\n",
       "      <td>25354.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>2012240.0</td>\n",
       "      <td>129350.0</td>\n",
       "      <td>61236.0</td>\n",
       "      <td>17084.0</td>\n",
       "      <td>11488.0</td>\n",
       "      <td>62462.0</td>\n",
       "      <td>49960.0</td>\n",
       "      <td>33932.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1005145.0</td>\n",
       "      <td>354328.0</td>\n",
       "      <td>37102.0</td>\n",
       "      <td>88881.0</td>\n",
       "      <td>45307.0</td>\n",
       "      <td>99603.0</td>\n",
       "      <td>44790.0</td>\n",
       "      <td>29749.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12806</th>\n",
       "      <td>99</td>\n",
       "      <td>-39.0</td>\n",
       "      <td>127574.0</td>\n",
       "      <td>9951.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>21732.0</td>\n",
       "      <td>3872.0</td>\n",
       "      <td>39728.0</td>\n",
       "      <td>2598.0</td>\n",
       "      <td>960.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12807</th>\n",
       "      <td>99</td>\n",
       "      <td>-275.0</td>\n",
       "      <td>323061.0</td>\n",
       "      <td>797464.0</td>\n",
       "      <td>153171.0</td>\n",
       "      <td>145805.0</td>\n",
       "      <td>39829.0</td>\n",
       "      <td>571280.0</td>\n",
       "      <td>36574.0</td>\n",
       "      <td>10010.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12808</th>\n",
       "      <td>99</td>\n",
       "      <td>-426.0</td>\n",
       "      <td>680989.0</td>\n",
       "      <td>154296.0</td>\n",
       "      <td>40068.0</td>\n",
       "      <td>39122.0</td>\n",
       "      <td>10966.0</td>\n",
       "      <td>26975.0</td>\n",
       "      <td>20427.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12809</th>\n",
       "      <td>99</td>\n",
       "      <td>-84.0</td>\n",
       "      <td>366269.0</td>\n",
       "      <td>27346.0</td>\n",
       "      <td>11444.0</td>\n",
       "      <td>9932.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>3283.0</td>\n",
       "      <td>12323.0</td>\n",
       "      <td>1764.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12810</th>\n",
       "      <td>99</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>1164555.0</td>\n",
       "      <td>1184366.0</td>\n",
       "      <td>50014.0</td>\n",
       "      <td>124208.0</td>\n",
       "      <td>10634.0</td>\n",
       "      <td>445383.0</td>\n",
       "      <td>22133.0</td>\n",
       "      <td>4482.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12811 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID    Raw      Delta      Theta    Alpha1    Alpha2    Beta1     Beta2  \\\n",
       "0       0  278.0   301963.0    90612.0   33735.0   23991.0  27946.0   45097.0   \n",
       "1       0  -50.0    73787.0    28083.0    1439.0    2240.0   2746.0    3687.0   \n",
       "2       0  101.0   758353.0   383745.0  201999.0   62107.0  36293.0  130536.0   \n",
       "3       0   -5.0  2012240.0   129350.0   61236.0   17084.0  11488.0   62462.0   \n",
       "4       0   -8.0  1005145.0   354328.0   37102.0   88881.0  45307.0   99603.0   \n",
       "...    ..    ...        ...        ...       ...       ...      ...       ...   \n",
       "12806  99  -39.0   127574.0     9951.0     709.0   21732.0   3872.0   39728.0   \n",
       "12807  99 -275.0   323061.0   797464.0  153171.0  145805.0  39829.0  571280.0   \n",
       "12808  99 -426.0   680989.0   154296.0   40068.0   39122.0  10966.0   26975.0   \n",
       "12809  99  -84.0   366269.0    27346.0   11444.0    9932.0   1939.0    3283.0   \n",
       "12810  99  -49.0  1164555.0  1184366.0   50014.0  124208.0  10634.0  445383.0   \n",
       "\n",
       "        Gamma1   Gamma2  predefinedlabel  \n",
       "0      33228.0   8293.0              0.0  \n",
       "1       5293.0   2740.0              0.0  \n",
       "2      57243.0  25354.0              0.0  \n",
       "3      49960.0  33932.0              0.0  \n",
       "4      44790.0  29749.0              0.0  \n",
       "...        ...      ...              ...  \n",
       "12806   2598.0    960.0              1.0  \n",
       "12807  36574.0  10010.0              1.0  \n",
       "12808  20427.0   2024.0              1.0  \n",
       "12809  12323.0   1764.0              1.0  \n",
       "12810  22133.0   4482.0              1.0  \n",
       "\n",
       "[12811 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = Path(\"/home/aseliverstov/projects/brain_signals/data_confusion\")\n",
    "data = pd.read_csv(data_dir / \"EEG_data.csv\")\n",
    "\n",
    "data[\"ID\"] = (len(np.unique(data[\"VideoID\"])) * data[\"SubjectID\"] + data[\"VideoID\"]).astype(\"int\")\n",
    "data = data[ID + FEATURES + TARGET]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_dataset(data):\n",
    "    features = []\n",
    "    target = []\n",
    "    for cur_id in np.unique(data[ID].to_numpy()):\n",
    "        cur_id_data = data[data[ID].to_numpy() == cur_id]\n",
    "        target.append(np.mean(cur_id_data[TARGET].to_numpy()).astype(\"int\"))\n",
    "        features.append(cur_id_data[FEATURES].to_numpy())\n",
    "\n",
    "    features = pad_sequences(features)\n",
    "    return np.array(features), np.array(target)\n",
    "\n",
    "def pad_sequences(arrays, pad_value=0):\n",
    "    max_length = max(arr.shape[0] for arr in arrays)\n",
    "    padded_arrays = [\n",
    "        np.pad(\n",
    "            arr,\n",
    "            ((0, max_length - arr.shape[0]), (0, 0)),\n",
    "            mode='constant',\n",
    "            constant_values=pad_value)\n",
    "            for arr in arrays\n",
    "        ]\n",
    "    return np.stack(padded_arrays)\n",
    "\n",
    "def create_model(train):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(train.shape[1],)))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))  # Dropout is deterministic when seed is fixed\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=[\"accuracy\", AUC(name=\"auc\")])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 01:13:34.363637: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val Accuracy = 0.49 AUC = 0.49 Loss = 1.8\n",
      "Epoch 2: Val Accuracy = 0.5 AUC = 0.49 Loss = 1.37\n",
      "Epoch 3: Val Accuracy = 0.5 AUC = 0.5 Loss = 1.19\n",
      "Epoch 4: Val Accuracy = 0.49 AUC = 0.51 Loss = 1.09\n",
      "Epoch 5: Val Accuracy = 0.5 AUC = 0.51 Loss = 1.02\n",
      "Epoch 6: Val Accuracy = 0.49 AUC = 0.52 Loss = 0.97\n",
      "Epoch 7: Val Accuracy = 0.5 AUC = 0.52 Loss = 0.93\n",
      "Epoch 8: Val Accuracy = 0.5 AUC = 0.52 Loss = 0.9\n",
      "Epoch 9: Val Accuracy = 0.5 AUC = 0.53 Loss = 0.87\n",
      "Epoch 10: Val Accuracy = 0.5 AUC = 0.53 Loss = 0.86\n",
      "Epoch 11: Val Accuracy = 0.5 AUC = 0.53 Loss = 0.84\n",
      "Epoch 12: Val Accuracy = 0.5 AUC = 0.53 Loss = 0.83\n",
      "Epoch 13: Val Accuracy = 0.5 AUC = 0.54 Loss = 0.82\n",
      "Epoch 14: Val Accuracy = 0.5 AUC = 0.54 Loss = 0.81\n",
      "Epoch 15: Val Accuracy = 0.5 AUC = 0.54 Loss = 0.8\n",
      "Epoch 16: Val Accuracy = 0.5 AUC = 0.54 Loss = 0.8\n",
      "Epoch 17: Val Accuracy = 0.51 AUC = 0.55 Loss = 0.79\n",
      "Epoch 18: Val Accuracy = 0.5 AUC = 0.55 Loss = 0.79\n",
      "Epoch 19: Val Accuracy = 0.49 AUC = 0.55 Loss = 0.78\n",
      "Epoch 20: Val Accuracy = 0.51 AUC = 0.55 Loss = 0.78\n",
      "Epoch 21: Val Accuracy = 0.51 AUC = 0.55 Loss = 0.78\n",
      "Epoch 22: Val Accuracy = 0.51 AUC = 0.55 Loss = 0.77\n",
      "Epoch 23: Val Accuracy = 0.5 AUC = 0.55 Loss = 0.77\n",
      "Epoch 24: Val Accuracy = 0.5 AUC = 0.55 Loss = 0.77\n",
      "Epoch 25: Val Accuracy = 0.5 AUC = 0.55 Loss = 0.76\n",
      "Epoch 26: Val Accuracy = 0.5 AUC = 0.56 Loss = 0.76\n",
      "Epoch 27: Val Accuracy = 0.5 AUC = 0.56 Loss = 0.76\n",
      "Epoch 28: Val Accuracy = 0.49 AUC = 0.56 Loss = 0.76\n",
      "Epoch 29: Val Accuracy = 0.5 AUC = 0.56 Loss = 0.76\n",
      "Epoch 30: Val Accuracy = 0.5 AUC = 0.56 Loss = 0.75\n",
      "Epoch 31: Val Accuracy = 0.51 AUC = 0.56 Loss = 0.75\n",
      "Epoch 32: Val Accuracy = 0.51 AUC = 0.57 Loss = 0.75\n",
      "Epoch 33: Val Accuracy = 0.51 AUC = 0.56 Loss = 0.75\n",
      "Epoch 34: Val Accuracy = 0.51 AUC = 0.57 Loss = 0.75\n",
      "Epoch 35: Val Accuracy = 0.51 AUC = 0.57 Loss = 0.75\n",
      "Epoch 36: Val Accuracy = 0.51 AUC = 0.57 Loss = 0.75\n",
      "Epoch 37: Val Accuracy = 0.51 AUC = 0.58 Loss = 0.75\n",
      "Epoch 38: Val Accuracy = 0.51 AUC = 0.58 Loss = 0.75\n",
      "Epoch 39: Val Accuracy = 0.51 AUC = 0.58 Loss = 0.75\n",
      "Epoch 40: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.75\n",
      "Epoch 41: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.75\n",
      "Epoch 42: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.75\n",
      "Epoch 43: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.75\n",
      "Epoch 44: Val Accuracy = 0.51 AUC = 0.58 Loss = 0.75\n",
      "Epoch 45: Val Accuracy = 0.52 AUC = 0.59 Loss = 0.74\n",
      "Epoch 46: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.74\n",
      "Epoch 47: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.74\n",
      "Epoch 48: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.74\n",
      "Epoch 49: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.74\n",
      "Epoch 50: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.74\n",
      "Epoch 51: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.74\n",
      "Epoch 52: Val Accuracy = 0.53 AUC = 0.58 Loss = 0.74\n",
      "Epoch 53: Val Accuracy = 0.54 AUC = 0.58 Loss = 0.74\n",
      "Epoch 54: Val Accuracy = 0.53 AUC = 0.58 Loss = 0.74\n",
      "Epoch 55: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.74\n",
      "Epoch 56: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.74\n",
      "Epoch 57: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.74\n",
      "Epoch 58: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.74\n",
      "Epoch 59: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.75\n",
      "Epoch 60: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.74\n",
      "Epoch 61: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.74\n",
      "Epoch 62: Val Accuracy = 0.52 AUC = 0.58 Loss = 0.74\n",
      "Epoch 63: Val Accuracy = 0.52 AUC = 0.59 Loss = 0.74\n",
      "Epoch 64: Val Accuracy = 0.52 AUC = 0.59 Loss = 0.74\n",
      "Epoch 65: Val Accuracy = 0.52 AUC = 0.59 Loss = 0.74\n",
      "Epoch 66: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 67: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 68: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 69: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 70: Val Accuracy = 0.53 AUC = 0.58 Loss = 0.74\n",
      "Epoch 71: Val Accuracy = 0.52 AUC = 0.59 Loss = 0.74\n",
      "Epoch 72: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 73: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 74: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 75: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 76: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 77: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 78: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 79: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 80: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 81: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 82: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 83: Val Accuracy = 0.53 AUC = 0.59 Loss = 0.74\n",
      "Epoch 84: Val Accuracy = 0.54 AUC = 0.59 Loss = 0.74\n",
      "Epoch 85: Val Accuracy = 0.54 AUC = 0.59 Loss = 0.74\n",
      "Epoch 86: Val Accuracy = 0.54 AUC = 0.59 Loss = 0.74\n",
      "Epoch 87: Val Accuracy = 0.53 AUC = 0.6 Loss = 0.74\n",
      "Epoch 88: Val Accuracy = 0.54 AUC = 0.6 Loss = 0.74\n",
      "Epoch 89: Val Accuracy = 0.54 AUC = 0.6 Loss = 0.74\n",
      "Epoch 90: Val Accuracy = 0.54 AUC = 0.6 Loss = 0.74\n",
      "Epoch 91: Val Accuracy = 0.54 AUC = 0.6 Loss = 0.74\n",
      "Epoch 92: Val Accuracy = 0.54 AUC = 0.6 Loss = 0.74\n",
      "Epoch 93: Val Accuracy = 0.54 AUC = 0.6 Loss = 0.74\n",
      "Epoch 94: Val Accuracy = 0.54 AUC = 0.6 Loss = 0.74\n",
      "Epoch 95: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.74\n",
      "Epoch 96: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.74\n",
      "Epoch 97: Val Accuracy = 0.56 AUC = 0.6 Loss = 0.74\n",
      "Epoch 98: Val Accuracy = 0.56 AUC = 0.6 Loss = 0.74\n",
      "Epoch 99: Val Accuracy = 0.56 AUC = 0.6 Loss = 0.74\n",
      "Epoch 100: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.74\n",
      "Epoch 101: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.74\n",
      "Epoch 102: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.75\n",
      "Epoch 103: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.75\n",
      "Epoch 104: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.74\n",
      "Epoch 105: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.74\n",
      "Epoch 106: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.74\n",
      "Epoch 107: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.74\n",
      "Epoch 108: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.74\n",
      "Epoch 109: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.74\n",
      "Epoch 110: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.74\n",
      "Epoch 111: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.74\n",
      "Epoch 112: Val Accuracy = 0.56 AUC = 0.6 Loss = 0.74\n",
      "Epoch 113: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.75\n",
      "Epoch 114: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.75\n",
      "Epoch 115: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.75\n",
      "Epoch 116: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.75\n",
      "Epoch 117: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.75\n",
      "Epoch 118: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.75\n",
      "Epoch 119: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.75\n",
      "Epoch 120: Val Accuracy = 0.55 AUC = 0.6 Loss = 0.75\n",
      "Epoch 121: Val Accuracy = 0.55 AUC = 0.61 Loss = 0.74\n",
      "Epoch 122: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.74\n",
      "Epoch 123: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.75\n",
      "Epoch 124: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.75\n",
      "Epoch 125: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.75\n",
      "Epoch 126: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.75\n",
      "Epoch 127: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.75\n",
      "Epoch 128: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.74\n",
      "Epoch 129: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.74\n",
      "Epoch 130: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.74\n",
      "Epoch 131: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.75\n",
      "Epoch 132: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.75\n",
      "Epoch 133: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.75\n",
      "Epoch 134: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.75\n",
      "Epoch 135: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.75\n",
      "Epoch 136: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.75\n",
      "Epoch 137: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.75\n",
      "Epoch 138: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.75\n",
      "Epoch 139: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.75\n",
      "Epoch 140: Val Accuracy = 0.56 AUC = 0.6 Loss = 0.75\n",
      "Epoch 141: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.75\n",
      "Epoch 142: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.75\n",
      "Epoch 143: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.75\n",
      "Epoch 144: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.75\n",
      "Epoch 145: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.76\n",
      "Epoch 146: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.76\n",
      "Epoch 147: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.76\n",
      "Epoch 148: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.76\n",
      "Epoch 149: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.76\n",
      "Epoch 150: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.76\n",
      "Epoch 151: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.76\n",
      "Epoch 152: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.76\n",
      "Epoch 153: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.76\n",
      "Epoch 154: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.76\n",
      "Epoch 155: Val Accuracy = 0.58 AUC = 0.61 Loss = 0.76\n",
      "Epoch 156: Val Accuracy = 0.58 AUC = 0.61 Loss = 0.76\n",
      "Epoch 157: Val Accuracy = 0.58 AUC = 0.62 Loss = 0.76\n",
      "Epoch 158: Val Accuracy = 0.58 AUC = 0.62 Loss = 0.76\n",
      "Epoch 159: Val Accuracy = 0.57 AUC = 0.62 Loss = 0.76\n",
      "Epoch 160: Val Accuracy = 0.57 AUC = 0.62 Loss = 0.76\n",
      "Epoch 161: Val Accuracy = 0.57 AUC = 0.62 Loss = 0.76\n",
      "Epoch 162: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.76\n",
      "Epoch 163: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.76\n",
      "Epoch 164: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.76\n",
      "Epoch 165: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.77\n",
      "Epoch 166: Val Accuracy = 0.55 AUC = 0.61 Loss = 0.77\n",
      "Epoch 167: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.77\n",
      "Epoch 168: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.78\n",
      "Epoch 169: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.78\n",
      "Epoch 170: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.78\n",
      "Epoch 171: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.78\n",
      "Epoch 172: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.78\n",
      "Epoch 173: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.77\n",
      "Epoch 174: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.77\n",
      "Epoch 175: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.77\n",
      "Epoch 176: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.77\n",
      "Epoch 177: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.77\n",
      "Epoch 178: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.77\n",
      "Epoch 179: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.78\n",
      "Epoch 180: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.78\n",
      "Epoch 181: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.78\n",
      "Epoch 182: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.78\n",
      "Epoch 183: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.78\n",
      "Epoch 184: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.78\n",
      "Epoch 185: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.78\n",
      "Epoch 186: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.78\n",
      "Epoch 187: Val Accuracy = 0.56 AUC = 0.61 Loss = 0.78\n",
      "Epoch 188: Val Accuracy = 0.57 AUC = 0.61 Loss = 0.78\n",
      "Epoch 189: Val Accuracy = 0.57 AUC = 0.62 Loss = 0.78\n",
      "Epoch 190: Val Accuracy = 0.56 AUC = 0.62 Loss = 0.78\n",
      "Epoch 191: Val Accuracy = 0.56 AUC = 0.62 Loss = 0.78\n",
      "Epoch 192: Val Accuracy = 0.56 AUC = 0.62 Loss = 0.78\n",
      "Epoch 193: Val Accuracy = 0.57 AUC = 0.62 Loss = 0.78\n",
      "Epoch 194: Val Accuracy = 0.57 AUC = 0.62 Loss = 0.79\n",
      "Epoch 195: Val Accuracy = 0.57 AUC = 0.62 Loss = 0.79\n",
      "Epoch 196: Val Accuracy = 0.58 AUC = 0.62 Loss = 0.78\n",
      "Epoch 197: Val Accuracy = 0.58 AUC = 0.62 Loss = 0.79\n",
      "Epoch 198: Val Accuracy = 0.58 AUC = 0.62 Loss = 0.79\n",
      "Epoch 199: Val Accuracy = 0.57 AUC = 0.62 Loss = 0.79\n",
      "Epoch 200: Val Accuracy = 0.58 AUC = 0.62 Loss = 0.79\n"
     ]
    }
   ],
   "source": [
    "all_histories_acc = []\n",
    "all_histories_loss = []\n",
    "all_histories_auc = []\n",
    "\n",
    "for seed in [1212, 123343, 74432, 64342, 9665]:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(group_kfold.split(data[FEATURES], data[TARGET], data[ID])):\n",
    "        train = data.iloc[train_index]\n",
    "        test = data.iloc[test_index]\n",
    "\n",
    "        X_train, y_train = reshape_dataset(train)\n",
    "        X_test, y_test = reshape_dataset(test)\n",
    "\n",
    "        X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "        y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "        model = create_model(X_train_flat)\n",
    "        history = model.fit(\n",
    "            X_train_flat, y_train,\n",
    "            validation_data=(X_test_flat, y_test),\n",
    "            epochs=NUM_EPOCHS,\n",
    "            batch_size=16,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        all_histories_acc.append(history.history['val_accuracy'])\n",
    "        all_histories_loss.append(history.history['val_loss'])\n",
    "        all_histories_auc.append(history.history['val_auc'])\n",
    "\n",
    "avg_val_accuracy = np.mean(all_histories_acc, axis=0)\n",
    "avg_val_loss = np.mean(all_histories_loss, axis=0)\n",
    "avg_val_auc = np.mean(all_histories_auc, axis=0)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}: Val Accuracy = {np.round(avg_val_accuracy[epoch], 2)} AUC = {np.round(avg_val_auc[epoch], 2)} Loss = {np.round(avg_val_loss[epoch], 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_25\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_25\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_100 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,504</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_50          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_101 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_51          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_102 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_103 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_100 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m41,504\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_50          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_50 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_101 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_51          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_51 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_102 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_103 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,361</span> (169.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m43,361\u001b[0m (169.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,233</span> (168.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m43,233\u001b[0m (168.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain-signals-_5HxkjSc-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
